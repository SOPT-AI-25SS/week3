## ì‹¤ì‹œê°„ ìŒì„±-ê¸°ë°˜ íšŒì˜ ì¸ì‚¬ì´íŠ¸ í”Œë«í¼ êµ¬ì¶• **ì›Œí¬ìˆ ê¸°íšì•ˆ**

---

### 1. ì¶”ì§„ ë°°ê²½ Â· ëª©ì 
- **ì›ê²©Â·í•˜ì´ë¸Œë¦¬ë“œ ì—…ë¬´**ê°€ ì¼ìƒí™”ë˜ë©° íšŒì˜ ìŒì„± ë°ì´í„°ì˜ êµ¬ì¡°í™” Â· ê²€ìƒ‰ ìˆ˜ìš”ê°€ ê¸‰ì¦
- Google Cloud Vertex AI ìƒíƒœê³„ë¥¼ í™œìš©í•´ **â€œë…¹ìŒ-â†’ í…ìŠ¤íŠ¸-â†’ ì„ë² ë”©-â†’ RAG ì±—ë´‡â€** ì „ ê³¼ì •ì„ í•˜ë£¨ ë§Œì— ì²´í—˜í•˜ëŠ” **ì‹¤ìŠµ ì¤‘ì‹¬ êµìœ¡**ì„ ì œê³µ
- ìˆ˜ê°•ìƒì´ í˜„ì—…ì— ëŒì•„ê°€ **ìì‚¬ íšŒì˜ë¡ / ì½œì„¼í„° / êµìœ¡ ì½˜í…ì¸  ìë™í™”** PoCë¥¼ ì¦‰ì‹œ ì°©ìˆ˜í•  ìˆ˜ ìˆë„ë¡ ì½”ë“œ ìŠ¤ìºí´ë”© ì œê³µ

---

### 2. ê¸°ëŒ€ íš¨ê³¼
1. **E2E AI íŒŒì´í”„ë¼ì¸** ì‹¤ìŠµìœ¼ë¡œ Vertex AI, Speech-to-Text, Vector Search ì´í•´ë„ ìƒìŠ¹
2. ì‹¤ìŠµ ê²°ê³¼ë¬¼ì´ ê·¸ëŒ€ë¡œ íŒ€/íšŒì‚¬ PoCì˜ **ì´ˆì„(ì½”ë“œ + GCP ì…‹ì—…)** ì—­í• 
3. ë²¤ì¹˜ë§ˆí‚¹ì„ í†µí•´ **ë¹„ìš© ìµœì í™”Â·Latency ê°œì„ ** ë“± ì„±ëŠ¥ íŠœë‹ ë…¸í•˜ìš° ìŠµë“

---

### 3. ëª©í‘œ ì‹œìŠ¤í…œ ê¸°ëŠ¥
1. **ë¸Œë¼ìš°ì € ì‹¤ì‹œê°„ ìŒì„± ë…¹ìŒ** â€“ MediaRecorder API â†’ MP3 ì—…ë¡œë“œ
2. **ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜** â€“ Gemini Audio Understanding https://ai.google.dev/gemini-api/docs/audio ì‚¬ìš©í•˜ê¸°
```
import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp3",
    config: { mimeType: "audio/mp3" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Describe this audio clip",
    ]),
  });
  console.log(response.text);
}

await main();

import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const base64AudioFile = fs.readFileSync("path/to/small-sample.mp3", {
  encoding: "base64",
});

const contents = [
  { text: "Please summarize the audio." },
  {
    inlineData: {
      mimeType: "audio/mp3",
      data: base64AudioFile,
    },
  },
];

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: contents,
});
console.log(response.text);import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const result = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
    "Generate a transcript of the speech.",
  ]),
});
console.log("result.text=", result.text);

import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.0-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
  ]),
});
console.log(countTokensResponse.totalTokens);

// Create a prompt containing timestamps.
const prompt = "Provide a transcript of the speech from 02:30 to 03:29."

WAV - audio/wav
MP3 - audio/mp3
AIFF - audio/aiff
AAC - audio/aac
OGG Vorbis - audio/ogg
FLAC - audio/flac

Gemini represents each second of audio as 32 tokens; for example, one minute of audio is represented as 1,920 tokens.
Gemini can only infer responses to English-language speech.
Gemini can "understand" non-speech components, such as birdsong or sirens.
The maximum supported length of audio data in a single prompt is 9.5 hours. Gemini doesn't limit the number of audio files in a single prompt; however, the total combined length of all audio files in a single prompt can't exceed 9.5 hours.
Gemini downsamples audio files to a 16 Kbps data resolution.
If the audio source contains multiple channels, Gemini combines those channels into a single channel.
```
3. **í…ìŠ¤íŠ¸ ë¶„í•  & ì„ë² ë”©** â€“ Vertex AI Text-Embedding-005 ë˜ëŠ” Gemini Embedding
4. **ë²¡í„° ì¸ë±ì‹± & ê²€ìƒ‰** â€“ Vertex AI Vector Search (BigQuery ë²¡í„° ì»¬ëŸ¼ ì„ íƒ ê°€ëŠ¥)
5. **ì§ˆì˜-ì‘ë‹µ ì±—ë´‡(RAG)** â€“ Gemini 1.5 Pro + TOP-k ë¬¸ë‹¨ ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
6. **Dashboard** â€“ ì§ˆì˜-ì‘ë‹µ ê²°ê³¼ì™€ ê·¼ê±° ë¬¸ë‹¨ í•˜ì´ë¼ì´íŠ¸

---

### 4. ì›Œí¬ìˆ ê°œìš”

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ëŒ€ìƒ** | ê°œë°œìÂ·ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸Â·PM (ìµœëŒ€ 25ëª…) |
| **í˜•ì‹** | **6 ì‹œê°„ ì˜¤í”„ë¼ì¸ í•¸ì¦ˆ-ì˜¨ ë©** + ë¯¸ë‹ˆ í•´ì»¤í†¤ |
| **ì‚¬ì „ ì¤€ë¹„** | â‘  GCP í”„ë¡œì íŠ¸ 1ì¸ 1ê°œ, â‘¡ Cloud Shell, â‘¢ Chrome ê¸°ë°˜ ë…¸íŠ¸ë¶ |
| **ì œê³µ ìë£Œ** | - ì™„ì„± ì½”ë“œ ë ˆí¬(`/labs`)<br>- ë‹¨ê³„ë³„ README + TODO ì£¼ì„<br>- ìŠ¬ë¼ì´ë“œ(PDF) & ë…¹í™” ë§í¬ |

---

### 5. ì„¸ë¶€ ì»¤ë¦¬í˜ëŸ¼ (6 h)

| ì‹œê°„ | ë‚´ìš© | ì£¼ìš” ì•¡ì…˜ |
|------|------|----------|
| 09:30 â€“ 10:00 | Kick-off & í™˜ê²½ì…‹ì—… | IAMÂ·API Enable, ì €ì¥ì†Œ ë²„í‚· ìƒì„± |
| 10:00 â€“ 11:00 | **Lab 1 : ë¸Œë¼ìš°ì € ë…¹ìŒ & ì—…ë¡œë“œ** | HTML + Flask, Signed URL ì—…ë¡œë“œ |
| 11:00 â€“ 12:00 | **Lab 2 : STT ë³€í™˜** | ì¥ê¸° ìŒì„± ë¹„ë™ê¸° API í˜¸ì¶œ, WER ì²´í¬ |
| 13:00 â€“ 14:00 | **Lab 3 : í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ & ì„ë² ë”©** | 512 token ìŠ¬ë¼ì´ë”© ìœˆë„ìš°, Embedding í˜¸ì¶œ |
| 14:00 â€“ 15:00 | **Lab 4 : Vector Search êµ¬ì¶•** | Index ìƒì„±, ìœ ì‚¬ë„ TOP-k ì¿¼ë¦¬ |
| 15:00 â€“ 16:00 | **Lab 5 : Gemini RAG ì±—ë´‡** | í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿Â·ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ |
| 16:00 â€“ 16:30 | Demo & Wrap-up | ì„±ëŠ¥Â·ë¹„ìš© ë² ìŠ¤íŠ¸íŒ€ ì‹œìƒ |

---

### 6. ì‚°ì¶œë¬¼
- `record.html`, `backend_api.py`, `transcribe.py`, `embedding.py`, `rag_chat.py`
- GCP ìì›: Storage ë²„í‚·, Vertex AI Index, Endpoint
- README: ë°°í¬ ê°€ì´ë“œ + ë¹„ìš© ê³„ì‚° ìŠ¤í”„ë ˆë“œì‹œíŠ¸

---

### 7. ìì› Â· ì˜ˆì‚° (ì¸ë‹¹ 8 h ì‹¤ìŠµ ê¸°ì¤€)

| êµ¬ë¶„ | ì„¸ë¶€ | ì¶”ì • ë¹„ìš© |
|------|------|-----------|
| **Speech-to-Text** | 30 ë¶„ Ã— 2 íšŒ | â‰ˆ **\$0.36** |
| **Embedding** | 200 ë¬¸ë‹¨ | â‰ˆ **\$0.05** |
| **Vector Search** | 1 GB ì¸ë±ìŠ¤ / 1 ì¼ | â‰ˆ **\$0.15** |
| **Gemini 1.5 Pro** | 5 ì§ˆì˜ Ã— 8k í† í° | â‰ˆ **\$0.40** |
| **í•©ê³„** | (VAT ë³„ë„) | **\$0.96 / ì¸ë‹¹** |

â€» ì›Œí¬ìˆ ë°ëª¨ ìˆ˜ì¤€ìœ¼ë¡œ ì›” \$1 ë¯¸ë§Œ. ì‹¤ì œ ì„œë¹„ìŠ¤ ì „í™˜ ì‹œ ì‚¬ìš©ëŸ‰Â·Auto-Pause ì „ëµìœ¼ë¡œ ìµœì í™” í•„ìš”.

---

### 8. íŒ€ êµ¬ì„± Â· ì—­í• 
- **í…Œí¬ ë¦¬ë“œ(1)** : ì»¤ë¦¬í˜ëŸ¼ ìš´ì˜, ê¸°ìˆ  Q&A
- **íŠœí„°(3)** : ì‹¤ìŠµ ì¤‘ ì‹¤ì‹œê°„ ì½”ë“œ/ì½˜ì†” ì§€ì›
- **í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €(1)** : ì¼ì •Â·ì˜ˆì‚°Â·ì„±ê³¼ ê´€ë¦¬
- **ì°¸ê°€ì(â‰¤ 25)** : 4~5ì¸ ìŠ¤ì¿¼ë“œë¡œ ë¯¸ì…˜ ìˆ˜í–‰

---

### 9. ìœ„í—˜ ìš”ì†Œ Â· ëŒ€ì‘
| Risk | ì˜í–¥ | ëŒ€ì‘ ë°©ì•ˆ |
|------|------|-----------|
| API ì¿¼í„° ì´ˆê³¼ | ì‹¤ìŠµ ì¤‘ë‹¨ | ì›Œí¬ìˆìš© ë³„ë„ Billing í”„ë¡œì íŠ¸ ì‚¬ì „ í™•ë³´ |
| ë„¤íŠ¸ì›Œí¬ ë¶ˆì•ˆì • | ëª¨ë¸ í˜¸ì¶œ ì§€ì—° | ì˜¤í”„ë¼ì¸ Whisper Container ì˜ˆë¹„ ì œê³µ |
| í•™ìŠµ ë‚œì´ë„ í¸ì°¨ | ì¼ì • ì§€ì—° | ë¯¸ì…˜-ê¸°ë°˜ íŠœí† ë¦¬ì–¼, ì½”ë“œ ìŠ¤ìºí´ë”© ì œê³µ |

---

### 10. ì„±ê³µ ì§€í‘œ (ì›Œí¬ìˆ ì¢…ë£Œ ê¸°ì¤€)
1. **ë…¹ìŒ-â†’ ë‹µë³€ ì´ ì†Œìš” â‰¤ 30 ì´ˆ** ë‹¬ì„± íŒ€ ë¹„ìœ¨ 70 %â†‘
2. ì°¸ê°€ì ì„¤ë¬¸ â€“ â€œì—…ë¬´ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥â€ 4 ì /5 ì  ì´ìƒ 80 %â†‘
3. ì›Œí¬ìˆ ì´í›„ 1 ê°œì›” ë‚´ **PoC ì°©ìˆ˜íŒ€ 3íŒ€** ì´ìƒ

---

### 11. ì¶”ì§„ ì¼ì • (ì œì•ˆ)
- **D-14** : ì°¸ê°€ì ëª¨ì§‘Â·GCP ê³„ì • ë°œê¸‰
- **D-7** : ìë£ŒÂ·ì½”ë“œ Freeze, Dry-Run
- **D-Day** : ì›Œí¬ìˆ ì‹¤í–‰
- **D + 7** : ì„¤ë¬¸Â·í”¼ë“œë°±Â·ì†ŒìŠ¤ ì½”ë“œ ê³µê°œ
- **D + 30** : PoC ì»¨ì„¤íŒ… ì„¸ì…˜(í¬ë§ íŒ€ ëŒ€ìƒ)

---

### 12. ë¶€ë¡ â€” ê¶Œì¥ ë ˆí¬ êµ¬ì¡°
```
/labs
 â”œâ”€ 01_record_upload/
 â”œâ”€ 02_stt/
 â”œâ”€ 03_embedding/
 â”œâ”€ 04_vector_search/
 â””â”€ 05_rag_chat/
```
> ê° ë””ë ‰í„°ë¦¬ë³„ DockerfileÂ·requirements.txt í¬í•¨, `README.md` ì— ì‹¤ìŠµ ê°€ì´ë“œÂ·TODO ë§ˆí‚¹.

---

### ê²°ë¡ 
ë³¸ ì›Œí¬ìˆì€ **â€œì‹¤ì‹œê°„ ìŒì„± ë°ì´í„°ì˜ êµ¬ì¡°í™”Â·ê²€ìƒ‰Â·í™œìš©â€** ì´ë¼ëŠ” ì‹œì¥ ìš”êµ¬ë¥¼ Vertex AI ê¸°ë°˜ ìµœì‹  ìŠ¤íƒìœ¼ë¡œ í•˜ë£¨ ë§Œì— ì²´í—˜í•  ìˆ˜ ìˆëŠ” ì˜¬ì¸ì› í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. ì°¸ê°€ìëŠ” ì‹¤ìŠµì„ í†µí•´ ì™„ì „í•œ E2E íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•˜ê³ , ì´í›„ ìì‚¬ ì—…ë¬´ì— ê³§ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ **PoC Seed** ë¥¼ ì†ì— ë„£ê²Œ ë©ë‹ˆë‹¤.

### êµ¬í˜„ ê³„íš

ì•„ë˜ ë‹¨ê³„ë³„ ì˜ˆì‹œëŠ” Next.js(íƒ€ì…ìŠ¤í¬ë¦½íŠ¸) í™˜ê²½ì—ì„œ ìŒì„± ì—…ë¡œë“œ â†’ STT â†’ ì„ë² ë”© â†’ ë²¡í„° ê²€ìƒ‰ â†’ RAG ì±—ë´‡ ì™„ì„±ê¹Œì§€ êµ¬í˜„í•˜ëŠ” ì¼ë°˜ì ì¸ êµ¬ì¡°ë¥¼ ê°„ëµíˆ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤. (ì£¼ë¡œ Next.js 13 ë²„ì „ì˜ Route Handlers, Server Actions ë“±ì„
í™œìš©í•œë‹¤ê³  ê°€ì •)

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        1. í”„ë¡œì íŠ¸ ì´ˆê¸° ì…‹ì—…
           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           â€¢ í”„ë¡œì íŠ¸ ìƒì„±
             npx create-next-app@latest my-voice-rag --typescript

    â€¢ .env íŒŒì¼ì— API í‚¤ë‚˜ GCP ì„¤ì • ë³´ê´€
      GOOGLE_API_KEY=yourApiKeyHere
      GCP_PROJECT_ID=yourProjectId

    â€¢ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
      npm install @google/genai @google-cloud/storage

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    2. ìŒì„± ë…¹ìŒ ë° ì—…ë¡œë“œ
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ í´ë¼ì´ì–¸íŠ¸ì—ì„œ ìŒì„± ë…¹ìŒ (MediaRecorder)

        1. pages/record.tsx(í˜¹ì€ app/record/page.tsx)ì— ë¸Œë¼ìš°ì € ì¸¡ ë…¹ìŒ UI êµ¬ì„±
        2. Blobìœ¼ë¡œ ìˆ˜ì§‘ëœ MP3/WAV íŒŒì¼ì„ /api/uploadë¡œ ì—…ë¡œë“œ

    â€¢ /api/upload ë¼ìš°íŠ¸ (Route Handler ì˜ˆì‹œ)
      íŒŒì¼: app/api/upload/route.ts
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      import { NextRequest, NextResponse } from 'next/server';
      import { Storage } from '@google-cloud/storage';

      const storage = new Storage();
      const bucket = storage.bucket(process.env.GCS_BUCKET ?? '');

      export async function POST(req: NextRequest) {
        // 1) Read file from formData
        const data = await req.formData();
        const file = data.get('audioFile') as File;

        // 2) Upload to GCS
        const blob = bucket.file(file.name);
        await blob.save(Buffer.from(await file.arrayBuffer()), {
          contentType: file.type
        });

        return NextResponse.json({
          success: true,
          gcsPath: `gs://${bucket.name}/${file.name}`
        });

      }
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    3. ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜(STT)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ /api/transcribe ë¼ìš°íŠ¸ì—ì„œ MP3/WAVë¥¼ Gemini Audio Understanding API(ë˜ëŠ” Vertex AI Speech)ë¡œ ì „ì†¡
    â€¢ ë‹¨ì¼ íŒŒì¼ì— ëŒ€í•´ ë¹„ë™ê¸°ë¡œ ì „ì‚¬í•´ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë°›ìŒ

    íŒŒì¼: app/api/transcribe/route.ts
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    import { NextRequest, NextResponse } from 'next/server';
    import { GoogleGenAI, createUserContent, createPartFromUri } from '@google/genai';

    const ai = new GoogleGenAI({ apiKey: process.env.GOOGLE_API_KEY ?? '' });

    export async function POST(req: NextRequest) {
      const { gcsPath } = await req.json();

      // STT í˜¸ì¶œ: GCS URI â†’ Gemini
      const response = await ai.models.generateContent({
        model: 'gemini-2.0-flash',
        contents: createUserContent([
          createPartFromUri(gcsPath, 'audio/mpeg'),
          'Transcribe the audio into English text.'
        ])
      });

      return NextResponse.json({ transcript: response.text });
    }
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    4. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ & ì„ë² ë”©
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ ê¸´ ì „ì‚¬ ê²°ê³¼ë¥¼ Semantic Chunking
    ```
    import "dotenv/config";
    import { OpenAIEmbeddings } from "@langchain/openai";
    import { TextLoader } from "langchain/document_loaders/fs/text";
    import natural from "natural";
    import * as math from "mathjs";
    import { quantile } from "d3-array"; 
    
    interface SentenceObject {
    sentence: string;
    index: number;
    combined_sentence?: string;
    combined_sentence_embedding?: number[];
    distance_to_next?: number;
    }
    
    /**
    * Asynchronously loads a text file and returns its content as a string.
      *
      * This function creates an instance of `TextLoader` to load the document
      * specified by the given relative path. It assumes the document loader
      * returns an array of documents, and extracts the page content of the first
      * document in this array.
      *
      * @param {string} relativePath - The relative path to the text file that needs to be loaded.
      * @returns {Promise<string>} A promise that resolves with the content of the text file as a string.
      *
    */
    const loadTextFile = async (relativePath: string): Promise<string> => {
    const loader = new TextLoader(relativePath);
    const docs = await loader.load();
    const textCorpus = docs[0].pageContent;
    return textCorpus;
    };
    
    /**
    * Splits a given text corpus into an array of sentences.
      *
      * This function utilizes `natural.SentenceTokenizerNew` to tokenize the provided text corpus
      * into individual sentences. It's designed to accurately recognize sentence boundaries
      * and split the text accordingly. The tokenizer's efficiency and accuracy in identifying
      * sentence endings allow for reliable sentence segmentation, which is crucial for
      * text processing tasks that require sentence-level analysis.
      *
      * @param {string} textCorpus - The text corpus to be split into sentences.
      * @returns {string[]} An array of sentences extracted from the text corpus.
      *
      * @example
      * const text = "Hello world. This is a test text.";
      * const sentences = splitToSentences(text);
      * console.log(sentences); // Output: ["Hello world.", "This is a test text."]
        */
        const splitToSentences = (textCorpus: string): string[] => {
        const tokenizer = new natural.SentenceTokenizerNew();
        const sentences = tokenizer.tokenize(textCorpus);
        return sentences;
        };
    
    /**
    * Structures an array of sentences into an array of `SentenceObject`s, each enhanced with combined sentences based on a specified buffer size.
      *
      * This function iterates through each sentence in the input array, creating an object for each that includes the original sentence, its index, and a combined sentence. The combined sentence is constructed by concatenating neighboring sentences within a specified range (bufferSize) before and after the current sentence, facilitating contextual analysis or embeddings in subsequent processing steps.
      *
      * The `bufferSize` determines how many sentences before and after the current sentence are included in the `combined_sentence`. For example, with a `bufferSize` of 1, each `combined_sentence` will include the sentence itself, the one preceding it, and the one following it, as long as such sentences exist.
      *
      * @param {string[]} sentences - An array of sentences to be structured.
      * @param {number} [bufferSize=1] - The number of sentences to include before and after the current sentence when forming the combined sentence. Defaults to 1.
      * @returns {SentenceObject[]} An array of `SentenceObject`s, each containing the original sentence, its index, and a combined sentence that includes its neighboring sentences based on the specified `bufferSize`.
      *
      * @example
      * const sentences = ["Sentence one.", "Sentence two.", "Sentence three."];
      * const structuredSentences = structureSentences(sentences, 1);
      * console.log(structuredSentences);
      * // Output: [
      * //   { sentence: 'Sentence one.', index: 0, combined_sentence: 'Sentence one. Sentence two.' },
      * //   { sentence: 'Sentence two.', index: 1, combined_sentence: 'Sentence one. Sentence two. Sentence three.' },
      * //   { sentence: 'Sentence three.', index: 2, combined_sentence: 'Sentence two. Sentence three.' }
      * // ]
        */
        const structureSentences = (
        sentences: string[],
        bufferSize: number = 1
        ): SentenceObject[] => {
        const sentenceObjectArray: SentenceObject[] = sentences.map(
        (sentence, i) => ({
        sentence,
        index: i,
        })
        );
    
    sentenceObjectArray.forEach((currentSentenceObject, i) => {
    let combinedSentence = "";

    for (let j = i - bufferSize; j < i; j++) {
      if (j >= 0) {
        combinedSentence += sentenceObjectArray[j].sentence + " ";
      }
    }

    combinedSentence += currentSentenceObject.sentence + " ";

    for (let j = i + 1; j <= i + bufferSize; j++) {
      if (j < sentenceObjectArray.length) {
        combinedSentence += sentenceObjectArray[j].sentence;
      }
    }

    sentenceObjectArray[i].combined_sentence = combinedSentence.trim();
    });
    
    return sentenceObjectArray;
    };
    
    /**
    * Generates embeddings for combined sentences within a new array of SentenceObject items, based on the input array, attaching the embeddings to their respective objects.
      *
      * This function takes an array of SentenceObject items, creates a deep copy to maintain purity, and then filters to identify those with a `combined_sentence`.
      * It generates embeddings for these combined sentences in bulk using the OpenAIEmbeddings service. Each embedding is then attached to the corresponding SentenceObject
      * in the copied array as `combined_sentence_embedding`.
      *
      * The function is pure and does not mutate the input array. Instead, it returns a new array with updated properties.
      *
      * @param {SentenceObject[]} sentencesArray - An array of SentenceObject items, each potentially containing a `combined_sentence`.
      * @returns {Promise<SentenceObject[]>} A promise that resolves with a new array of SentenceObject items, with embeddings attached to those items that have a `combined_sentence`.
      *
      * @example
      * const sentencesArray = [
      *   { sentence: 'Sentence one.', index: 0, combined_sentence: 'Sentence one. Sentence two.' },
      *   // other SentenceObject items...
      * ];
      * generateAndAttachEmbeddings(sentencesArray)
      *   .then(result => console.log(result))
      *   .catch(error => console.error('Error generating embeddings:', error));
          */
          const generateAndAttachEmbeddings = async (
          sentencesArray: SentenceObject[]
          ): Promise<SentenceObject[]> => {
          /* Create embedding instance */
          const embeddings = new OpenAIEmbeddings();
    
    // Deep copy the sentencesArray to ensure purity
    const sentencesArrayCopy: SentenceObject[] = sentencesArray.map(
    (sentenceObject) => ({
    ...sentenceObject,
    combined_sentence_embedding: sentenceObject.combined_sentence_embedding
    ? [...sentenceObject.combined_sentence_embedding]
    : undefined,
    })
    );
    
    // Extract combined sentences for embedding
    const combinedSentencesStrings: string[] = sentencesArrayCopy
    .filter((item) => item.combined_sentence !== undefined)
    .map((item) => item.combined_sentence as string);
    
    // Generate embeddings for the combined sentences
    const embeddingsArray = await embeddings.embedDocuments(
    combinedSentencesStrings
    );
    
    // Attach embeddings to the corresponding SentenceObject in the copied array
    let embeddingIndex = 0;
    for (let i = 0; i < sentencesArrayCopy.length; i++) {
    if (sentencesArrayCopy[i].combined_sentence !== undefined) {
    sentencesArrayCopy[i].combined_sentence_embedding =
    embeddingsArray[embeddingIndex++];
    }
    }
    
    return sentencesArrayCopy;
    };
    
    /**
    * Calculates the cosine similarity between two vectors.
      *
      * This function computes the cosine similarity between two vectors represented as arrays of numbers.
      * Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that
      * measures the cosine of the angle between them. The cosine of 0Â° is 1, and it is less than 1 for any other angle.
      * It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity
      * of 1, two vectors at 90Â° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1,
      * independent of their magnitude. Cosine similarity is particularly used in positive space, where the outcome is
      * neatly bounded in [0,1].
      *
      * The function returns 0 if either vector has a norm of 0.
      *
      * @param {number[]} vecA - The first vector, represented as an array of numbers.
      * @param {number[]} vecB - The second vector, also represented as an array of numbers.
      * @returns {number} The cosine similarity between vecA and vecB, a value between -1 and 1. Returns 0 if either vector's norm is 0.
      *
      * @example
      * const vectorA = [1, 2, 3];
      * const vectorB = [4, 5, 6];
      * const similarity = cosineSimilarity(vectorA, vectorB);
      * console.log(similarity); // Output: similarity score as a number
        */
        const cosineSimilarity = (vecA: number[], vecB: number[]): number => {
        const dotProduct = math.dot(vecA, vecB) as number;
    
    const normA = math.norm(vecA) as number;
    const normB = math.norm(vecB) as number;
    
    if (normA === 0 || normB === 0) {
    return 0;
    }
    
    const similarity = dotProduct / (normA * normB);
    return similarity;
    };
    
    /**
    * Enhances an array of SentenceObject items by calculating cosine distances between sentences and identifying significant semantic shifts based on a specified percentile threshold.
      * This function first calculates the cosine distance between each sentence's embedding and its next sentence's embedding. It then identifies which of these distances exceed a specified percentile threshold, indicating significant semantic shifts. The `distance_to_next` property is updated for each SentenceObject, and the indices of sentences where significant shifts occur are returned.
      * This operation is performed in a pure manner, ensuring the input array is not modified.
      *
      * @param {SentenceObject[]} sentenceObjectArray - An array of SentenceObject items, each containing a combined sentence embedding.
      * @param {number} percentileThreshold - The percentile threshold as a number (0-100) to identify significant semantic shifts.
      * @returns {{updatedArray: SentenceObject[], significantShiftIndices: number[]}} An object containing the updated array of SentenceObject items with `distance_to_next` property set, and an array of indices indicating significant semantic shifts.
      *
    */
    const calculateCosineDistancesAndSignificantShifts = (
    sentenceObjectArray: SentenceObject[],
    percentileThreshold: number
    ): { updatedArray: SentenceObject[]; significantShiftIndices: number[] } => {
    // Calculate cosine distances and update the array
    const distances: number[] = [];
    const updatedSentenceObjectArray = sentenceObjectArray.map(
    (item, index, array) => {
    if (
    index < array.length - 1 &&
    item.combined_sentence_embedding &&
    array[index + 1].combined_sentence_embedding
    ) {
    const embeddingCurrent = item.combined_sentence_embedding!;
    const embeddingNext = array[index + 1].combined_sentence_embedding!;
    const similarity = cosineSimilarity(embeddingCurrent, embeddingNext);
    const distance = 1 - similarity;
    distances.push(distance); // Keep track of calculated distances
    return { ...item, distance_to_next: distance };
    } else {
    return { ...item, distance_to_next: undefined };
    }
    }
    );
    
    // Determine the threshold value for significant shifts
    const sortedDistances = [...distances].sort((a, b) => a - b);
    const quantileThreshold = percentileThreshold / 100;
    const breakpointDistanceThreshold = quantile(
    sortedDistances,
    quantileThreshold
    );
    
    if (breakpointDistanceThreshold === undefined) {
    throw new Error("Failed to calculate breakpoint distance threshold");
    }
    
    // Identify indices of significant shifts
    const significantShiftIndices = distances
    .map((distance, index) =>
    distance > breakpointDistanceThreshold ? index : -1
    )
    .filter((index) => index !== -1);
    
    return {
    updatedArray: updatedSentenceObjectArray,
    significantShiftIndices,
    };
    };
    
    /**
    * Groups sentences into semantic chunks based on specified shift indices.
      *
      * This function accumulates sentences into chunks, where each chunk is defined by significant semantic shifts indicated by the provided shift indices. Each chunk comprises sentences that are semantically related, and the boundaries are determined by the shift indices, which point to sentences where a significant semantic shift occurs.
      *
      * @param {SentenceObject[]} sentenceObjectArray - An array of SentenceObject items, each potentially containing a sentence, its embedding, and additional metadata.
      * @param {number[]} shiftIndices - An array of indices indicating where significant semantic shifts occur, thus where new chunks should start.
      * @returns {string[]} An array of string, where each string is a concatenated group of semantically related sentences.
      *
      * @example
      * const sentencesWithEmbeddings = [
      *   { sentence: 'Sentence one.', index: 0 },
      *   // other SentenceObject items...
      * ];
      * const shiftIndices = [2, 5]; // Semantic shifts occur after the sentences at indices 2 and 5
      * const semanticChunks = groupSentencesIntoChunks(sentencesWithEmbeddings, shiftIndices);
      * console.log(semanticChunks); // Output: Array of concatenated sentence groups
        */
        const groupSentencesIntoChunks = (
        sentenceObjectArray: SentenceObject[],
        shiftIndices: number[]
        ): string[] => {
        let startIdx = 0; // Initialize the start index
        const chunks: string[] = []; // Create an array to hold the grouped sentences
    
    // Add one beyond the last index to handle remaining sentences as a final chunk
    const adjustedBreakpoints = [...shiftIndices, sentenceObjectArray.length - 1];
    
    // Iterate through the breakpoints to slice and accumulate sentences into chunks
    adjustedBreakpoints.forEach((breakpoint) => {
    // Extract the sentences from the current start index to the breakpoint (inclusive)
    const group = sentenceObjectArray.slice(startIdx, breakpoint + 1);
    const combinedText = group.map((item) => item.sentence).join(" "); // Combine the sentences
    chunks.push(combinedText);
    
        startIdx = breakpoint + 1; // Update the start index for the next group
    });
    
    return chunks;
    };
    
    async function main() {
    try {
    // Step 1: Load a text file.
    const textCorpus = await loadTextFile("assets/essaySmall.txt");

    // Step 2: Split the loaded text into sentences.
    const sentences = splitToSentences(textCorpus);

    // Step 3: Structure these sentences into an array of SentenceObject.
    const structuredSentences = structureSentences(sentences, 1); // Assuming a bufferSize of 1 for simplicity

    // Step 4: Generate embeddings for these combined sentences.
    const sentencesWithEmbeddings = await generateAndAttachEmbeddings(
      structuredSentences
    );

    // Step 5: Calculate cosine distances and significant shifts to identify semantic chunks.
    const { updatedArray, significantShiftIndices } =
      calculateCosineDistancesAndSignificantShifts(sentencesWithEmbeddings, 90); // Assuming a threshold of 90%

    // Step 6: Group sentences into semantic chunks based on the significant shifts identified.
    const semanticChunks = groupSentencesIntoChunks(
      updatedArray,
      significantShiftIndices
    );

    // Step 7: Log each semantic chunk with a clear separator.
    console.log("Semantic Chunks:\n");
    semanticChunks.forEach((chunk, index) => {
      console.log(`Chunk #${index + 1}:`);
      console.log(chunk);
      console.log("\n--------------------------------------------------\n");
    });
    ```
    â€¢ ê° ì²­í¬ ë¬¸ìì—´ì„ Embedding APIë¡œ í˜¸ì¶œ â†’ ì„ë² ë”© ë²¡í„° ìƒì„±
    â€¢ ì˜ˆì‹œ: /api/embedding ë¼ìš°íŠ¸ì—ì„œ transcriptë¥¼ ë°›ì•„ ì²˜ë¦¬

    íŒŒì¼: app/api/embedding/route.ts
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    import { NextRequest, NextResponse } from 'next/server';
    import { GoogleGenAI } from '@google/genai';

    const ai = new GoogleGenAI({ apiKey: process.env.GOOGLE_API_KEY ?? '' });

    export async function POST(req: NextRequest) {
      const { transcriptChunks } = await req.json();
      // transcriptChunks: string[] (ë¶„í• ëœ í…ìŠ¤íŠ¸ ë°°ì—´)

      const embeddings = [];
      for (const chunk of transcriptChunks) {
        const response = await ai.models.embed({
          model: 'textembedding-gecko', // ì˜ˆ: Vertex AI textembedding-gecko or gemini embedding
          texts: [chunk]
        });
        embeddings.push({ chunk, vector: response.embeddings[0] });
      }

      return NextResponse.json({ embeddings });
    }
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    5. ë²¡í„° ê²€ìƒ‰
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ ìƒì„±ëœ embeddingsë¥¼ Vertex AI Vector Search/Indexì— ì €ì¥
    â€¢ /api/index ë¼ìš°íŠ¸ ë“±ì—ì„œ â€œë¬¸ìì—´ ì¿¼ë¦¬ â†’ ì„ë² ë”© â†’ TOP-k ìœ ì‚¬ë„ ê²€ìƒ‰â€ êµ¬ì¡°
    â€¢ Read step5.md to implement.

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    6. RAG ì±—ë´‡ êµ¬í˜„
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ /api/chat ë¼ìš°íŠ¸ì—ì„œ ë‹¤ìŒ ì‘ì—… ìˆ˜í–‰:

        1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì„ë² ë”© ê³„ì‚°
        2. ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ Top-k ë¬¸ë‹¨/ìŠ¤ë‹ˆí« ì°¾ê¸°
        3. ì´ ìŠ¤ë‹ˆí«ë“¤ì„ í”„ë¡¬í”„íŠ¸ì— ì²¨ë¶€í•˜ì—¬ Gemini ëª¨ë¸ ë“±ìœ¼ë¡œ Q&A í˜¸ì¶œ
        4. ìµœì¢… ë‹µë³€ì„ ë°˜í™˜

    ì˜ˆ: /api/chat/route.ts
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    import { NextRequest, NextResponse } from 'next/server';
    import { GoogleGenAI } from '@google/genai';

    const ai = new GoogleGenAI({ apiKey: process.env.GOOGLE_API_KEY ?? '' });

    // (ê°€ì •) embeddingStore ëŠ” ì™„ì„±ëœ Vector Search ëª¨ë“ˆ or DB Client
    import { embeddingStore } from '@/lib/embeddingStore';

    export async function POST(req: NextRequest) {
      const { query } = await req.json();

      // 1) ì¿¼ë¦¬ ì„ë² ë”© êµ¬í•˜ê¸°
      const queryEmbeddingRes = await ai.models.embed({
        model: 'textembedding-gecko',
        texts: [query]
      });
      const queryEmbedding = queryEmbeddingRes.embeddings[0];

      // 2) ìœ ì‚¬ë„ ê²€ìƒ‰ â†’ topChunks
      const topChunks = await embeddingStore.search(queryEmbedding);

      // 3) í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM í˜¸ì¶œ
      const context = topChunks.map((c: any) => c.chunk).join('\n');
      const prompt = [
        You are an AI assistant. Use the following context to answer the question.\nContext:\n${context},
        Question: ${query}\nAnswer:
      ].join('\n');

      const answerRes = await ai.models.generateText({
        model: 'gemini-1.5-pro', // RAGì— ì í•©í•œ ëª¨ë¸
        prompt
      });

      return NextResponse.json({ answer: answerRes.text });
    }
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    7. í”„ë¡ íŠ¸ì—”ë“œ (ì±— UI ë° Dashboard)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ ë¸Œë¼ìš°ì €ì—ì„œëŠ” /api/chatê³¼ í†µì‹ í•˜ë©´ì„œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ëª¨ë¸ ë‹µë³€ì„ êµí™˜
    â€¢ ë‹µë³€ì— ê·¼ê±°ê°€ ëœ â€œtopChunksâ€ëŠ” extra ë‹¨ê³„ì—ì„œ ì¶”ê°€ë¡œ ë‚´ë ¤ ì‚¬ìš©ìì—ê²Œ ì¶œì²˜ë¥¼ í•˜ì´ë¼ì´íŠ¸í•  ìˆ˜ë„ ìˆìŒ
    â€¢ Next.jsì˜ Server Componentsë‚˜ Client Componentsë¥¼ í˜¼í•©í•´ êµ¬ì¡°ì ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìœ ì§€

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ì •ë¦¬
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ìœ„ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ Next.js íƒ€ì…ìŠ¤í¬ë¦½íŠ¸ í™˜ê²½ì—ì„œ ìŒì„± ê¸°ë°˜ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    â€¢ ë…¹ìŒí•˜ê³  â†’ ì—…ë¡œë“œ â†’ STT â†’ ì „ì²˜ë¦¬ & ì„ë² ë”© â†’ ë²¡í„° ê²€ìƒ‰ â†’ ë‹µë³€ ìƒì„±(RAG)
    â€¢ ê° ë‹¨ê³„ë¥¼ /api/â€¦ ë¼ìš°íŠ¸ë¡œ ì‘ê²Œ ë‚˜ëˆ„ì–´ ìœ ì§€ë³´ìˆ˜ì„±ê³¼ í™•ì¥ì„±ì„ í™•ë³´
    â€¢ GCP ìì›(Cloud Storage, Vertex AI, BigQuery, etc.)ì„ ì ì ˆíˆ ì—°ê²°í•´ ì‹¤ì œ PoCë‚˜ ìš´ì˜ í™˜ê²½ì— ì ìš©

    ì´ë ‡ê²Œ í•˜ë©´ README.mdì—ì„œ ì œì‹œëœ ì›Œí¬ìˆ ì‹œë‚˜ë¦¬ì˜¤(ìŒì„± ë…¹ìŒ ~ ì±—ë´‡)ì— ëŒ€ì‘í•˜ëŠ” End-to-End êµ¬í˜„ì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.

---

## ğŸ“„ STORM Parse â€“ Document â†’ Markdown Conversion API

This repository now also supports **STORM Parse**, a Retrieval-Augmented Generation (RAG) endpoint that converts uploaded documents (PDF or image) into Markdown. The endpoint is fully self-contained and follows the same clean, modular principles used for the existing voice-to-RAG pipeline.

### 1. Endpoint summary

```
POST /convert/rag
```

|                | Value                                            |
|----------------|--------------------------------------------------|
| **Purpose**    | Convert a document to Markdown with RAG prompt. |
| **Auth**       | HTTP header **`storm-api-key: <token>`**         |
| **Content-Type** | `multipart/form-data`                            |
| **Allowed files** | `pdf`, `png`, `jpg`, `jpeg`                     |

### 2. Request

```
POST /convert/rag HTTP/1.1
Host: <your-domain>
storm-api-key: ********************
Content-Type: multipart/form-data; boundary=----WebKitFormBoundary

------WebKitFormBoundary
Content-Disposition: form-data; name="file"; filename="sample.pdf"
Content-Type: application/pdf

<binary-data>
------WebKitFormBoundary--
```

#### Body parameters

| Field | Type | Required | Notes |
|-------|------|----------|-------|
| `file` | File | Yes | PDF / PNG / JPG / JPEG only |

### 3. Response

`HTTP 200 OK`

```json
{
  "pages": [
    {
      "page_number": 1,
      "content": "Markdown content of page 1"
    },
    {
      "page_number": 2,
      "content": "Markdown content of page 2"
    }
  ]
}
```

#### Error codes

| Status | Meaning                                   |
|--------|-------------------------------------------|
| 400    | Invalid file type or missing `file` field |
| 500    | Internal error during processing          |

### 4. cURL example

```bash
curl -X POST \
  -H "storm-api-key: $STORM_API_KEY" \
  -F "file=@/path/to/document.pdf" \
  https://<your-domain>/convert/rag
```

### 5. JavaScript (fetch) example

```ts
async function convertDocument(file: File): Promise<void> {
  const form = new FormData();
  form.append('file', file);

  const res = await fetch('/convert/rag', {
    method: 'POST',
    headers: { 'storm-api-key': process.env.NEXT_PUBLIC_STORM_API_KEY! },
    body: form,
  });

  if (!res.ok) throw new Error(`Failed â€“ ${res.status}`);
  const data = await res.json();
  console.log(data.pages);
}
```

### 6. Implementation guide (my-voice-rag)

1. **Create API route** â€“ `app/api/convert/rag/route.ts`
   â€¢ Parse `multipart/form-data` (see `busboy` or `formidable`).
   â€¢ Validate MIME type and size.
2. **Load STORM API key** â€“ add `STORM_API_KEY="â€¦"` to `.env` (exposed to runtime only) and proxy through the `storm-api-key` header to downstream services if needed.
3. **Vision â†’ Markdown** â€“
   â€¢ Pass the raw file to your Vision LLM (Gemini 1.5 Pro or similar).
   â€¢ Prompt pattern: _â€œConvert every element (text, table, image) to Markdown. Output **only** valid Markdown.â€_
   â€¢ Split multi-page documents and stream per-page responses for UX responsiveness.
4. **Return JSON payload** exactly as specified above.
5. **(Optional) Vector-index pages** â€“ Re-use `/api/vector-index` to embed & store each Markdown page so that the existing RAG chat endpoint can immediately query the converted document.

The new endpoint has zero coupling with the existing audio pipeline and can be iterated or scaled independently.

---

> Need help? Open an issue or ping the maintainer listed in `package.json`.
